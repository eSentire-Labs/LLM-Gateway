# This docker-compose file starts up the chat client

version: '3.8'

services:
  frontend:
    container_name: llm-gateway-chat
    build: ./
    environment:
      - LLM_GATEWAY_URL=http://llm-gateway-api:8000 # Replace this with your internal LLM Gateway Proxy url. This default value is currently pointing to the backend api docker container
    networks: 
      - llm-demo
    ports:
      - 3000:3000

networks:
  llm-demo: 