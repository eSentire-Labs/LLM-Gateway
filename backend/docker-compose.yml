# This docker-compose file starts up the api and a docker postgres container

version: '3.8'

services:
  api:
    container_name: llm-gateway-api
    build: .
    command: bash -c 'cd ./backend; uvicorn src.app:app --host 0.0.0.0'
    volumes:
      - .:/app
    ports:
      - 8008:8000
    environment:
      - DATABASE_URL=postgresql://fastapi_traefik:fastapi_traefik@db:5432/fastapi_traefik
      - LLM_API_CONTENT_TYPE=application/json
      - LLM_ENDPOINT=https://api.openai.com/v1/chat/completions # LLM endpoint you want to proxy to
      - LLM_IMG_ENDPOINT=https://api.openai.com/v1/images/generations # Another llm endpoint you want to proxy to
      - LLM_API_AUTHORIZATION=[LLM TOKEN/API KEY HERE. E,g. sk-asda...]
      - ENABLE_SUBMISSIONS_API=false # True of an API is enabled to send logs to eSentire
      - SUBMISSIONS_API_URL=[SUBMISSIONS URL PROVIDED IF CONFIGURED] # The URL provided by eSentire if llm log submissions is enabled
    networks:
      - llm-demo
    depends_on:
      db:
        condition: service_healthy
  db:
    container_name: llm-gateway-db
    image: postgres:15-alpine
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    expose:
      - 5432
    environment:
      - POSTGRES_USER=fastapi_traefik
      - POSTGRES_PASSWORD=fastapi_traefik
      - POSTGRES_DB=fastapi_traefik
    networks:
      - llm-demo
    healthcheck: # This health check is necessary so that we can accurately determin that the service is ready before starting the api
      test: ["CMD-SHELL", "pg_isready -U fastapi_traefik"] 
      interval: 10s
      timeout: 5s
      retries: 5
  
volumes:
  postgres_data:

networks:
  llm-demo: 